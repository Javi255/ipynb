{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Spacy\n",
        "[Curso Spacy](https://course.spacy.io/es/)"
      ],
      "metadata": {
        "id": "KVV6p_kZB9YO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lKRWfSFBnL8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a31a11b7-3307-40a8-902e-c385eeef85c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        }
      ],
      "source": [
        "# Importamos la librería spacy\n",
        "import spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encontrando palabras, frases, nombres y conceptos"
      ],
      "metadata": {
        "id": "GmnWCgw0W5TF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Objetos"
      ],
      "metadata": {
        "id": "w-TlfppICU-l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### NLP"
      ],
      "metadata": {
        "id": "EWsDl97JCQtR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos un objeto nlp vacío para procesar español\n",
        "nlp = spacy.blank(\"es\")"
      ],
      "metadata": {
        "id": "LvBC4Cd7B4ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### DOC"
      ],
      "metadata": {
        "id": "Gn8vwfroCTA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos un doc para procesar un string de texto con el objeto NLP\n",
        "doc1 = nlp(\"Hola, mi nombre es Javier y tengo 20 años\")\n",
        "\n",
        "# Mostramos todos los tokens línea a línea\n",
        "for token in doc1:\n",
        "    print(token.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-8ZGalSCHHy",
        "outputId": "4d56f532-510a-408e-a6d8-8e8053a2d9af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hola\n",
            ",\n",
            "mi\n",
            "nombre\n",
            "es\n",
            "Javier\n",
            "y\n",
            "tengo\n",
            "20\n",
            "años\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Insertamos el tercer token en una variable\n",
        "token = doc1[2]\n",
        "\n",
        "# Imprimimos la variable con el atributo .text\n",
        "print(token.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Oxrr5YSCN42",
        "outputId": "add92ae1-def4-49c4-9410-8e162289a6fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SPAN"
      ],
      "metadata": {
        "id": "AiY15-z7KVJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtenemos del tercer al sexto token\n",
        "span = doc1[2:6]\n",
        "\n",
        "# Imprimimos la variable gracias al atributo .text\n",
        "print(span.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cw0P4pTCKWjv",
        "outputId": "986676f8-4f6b-4044-d44b-06a8a3934927"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mi nombre es Javier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Atributos léxicos"
      ],
      "metadata": {
        "id": "85tTOXB4Loi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprime todas las posiciones disponibles\n",
        "print(\"Total posiciones: \", [token.i for token in doc1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVMyZqxeLp1X",
        "outputId": "2bad2e37-fc28-48ec-94dc-2c27241bb581"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Posiciones:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprime todo el texto\n",
        "print(\"Total texto: \", [token.text for token in doc1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snTV6SlSL6nO",
        "outputId": "9bd33fca-8d08-4908-d9d1-3718157622e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto:  ['Hola', ',', 'mi', 'nombre', 'es', 'Javier', 'y', 'tengo', '20', 'años']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Devuelve True o False si los tokens son alfanuméricos o no\n",
        "print(\"Alfanuméricos:\", [token.is_alpha for token in doc1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRk35jJEMEmN",
        "outputId": "50414f14-5b79-4303-8ad9-ea356043d623"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alfanuméricos: [True, False, True, True, True, True, True, True, False, True]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Devuelve True o False si los tokens son signos de puntuación o no\n",
        "print(\"Signos de puntuación:\", [token.is_punct for token in doc1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hjaVt_ZMM0_",
        "outputId": "e9c3bb31-5e2b-41b0-a444-eb3f8c3e0e7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Signos de puntuación: [False, True, False, False, False, False, False, False, False, False]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Devuelve True o False si los tokens son números o no\n",
        "print(\"Números:\", [token.like_num for token in doc1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VAeNYNBMQfn",
        "outputId": "b6d90842-60f5-4532-c976-1c55aa4d5469"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Números: [False, False, False, False, False, False, False, False, True, False]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pipelines entrenados"
      ],
      "metadata": {
        "id": "iqnpgH4INS13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Descargamos este pipeline preentrenado\n",
        "! python -m spacy download es_core_news_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HehXuS1CNUmt",
        "outputId": "27bb7056-d9a9-4b92-9a20-f974e884672c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "2023-03-07 17:27:24.623548: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-03-07 17:27:24.623653: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-03-07 17:27:24.623672: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2023-03-07 17:27:26.308883: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting es-core-news-sm==3.4.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.4.0/es_core_news_sm-3.4.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from es-core-news-sm==3.4.0) (3.4.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (4.64.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.1.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.0.12)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.0.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.0.8)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.25.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (6.3.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.4.6)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (8.1.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (23.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.0.7)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.0.9)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.22.4)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.10.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.26.14)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2022.12.7)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.1.2)\n",
            "Installing collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-3.4.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos el pipeline en un objeto nlp\n",
        "nlp = spacy.load(\"es_core_news_sm\")"
      ],
      "metadata": {
        "id": "90Qtrn6WNZhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Atributos:\n",
        "* .text (Imprime el texto)\n",
        "* .pos_ (Imprime la etiqueta gramatical)\n",
        "* .dep_ (Imprime la etiqueta de la dependencia sintáctica)\n",
        "* .head.text (Devuelve el token \"padre\" de la palabra)"
      ],
      "metadata": {
        "id": "_B_sPWDyeXOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc2 = nlp(\"Mi casa es azul\")\n",
        "\n",
        "for token in doc2:\n",
        "    print(token.text, token.pos_, token.dep_, token.head.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8WShLChOK6W",
        "outputId": "e5f97105-d656-4ddc-82f6-f41e43b536b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mi DET nsubj azul\n",
            "casa NOUN flat Mi\n",
            "es AUX cop azul\n",
            "azul ADJ ROOT azul\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ingresamos un texto con entidades\n",
        "doc3 = nlp(\"Seat es una marca de coches de Alemania que proviene de Volkswagen\")\n",
        "\n",
        "for entidad in doc3.ents:\n",
        "    # Imprime en pantalla el texto y la etiqueta de la entidad\n",
        "    print(entidad.text, entidad.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvjWKdPdOZMI",
        "outputId": "3c7ef0fa-8fae-432e-d9ee-ab520c6bcd52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seat ORG\n",
            "Alemania LOC\n",
            "Volkswagen ORG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Patrones basados en reglas"
      ],
      "metadata": {
        "id": "hkEVmY6jSb52"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Casos de uso"
      ],
      "metadata": {
        "id": "IcV_rEVpTnwO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aquí estamos buscando tres tokens con el texto \"camiseta\", \"blanca\" y \"rayas\".\n",
        "[{\"TEXT\": \"camiseta\"}, {\"TEXT\": \"blanca\"}, {\"TEXT\": \"rayas\"}]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9Gwl86ISd7_",
        "outputId": "f7d6c8e7-8061-41ea-8261-2bf2ac696130"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'TEXT': 'camiseta'}, {'TEXT': 'blanca'}, {'TEXT': 'rayas'}]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Aquí estamos buscando dos tokens que en minúsculas sean \"camiseta\" y \"blanca\".\n",
        "[{\"LOWER\": \"camiseta\"}, {\"LOWER\": \"blanca\"}]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tdtum4SfS24u",
        "outputId": "c2f8e20d-1006-427b-94d8-39f943894374"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'LOWER': 'camiseta'}, {'LOWER': 'blanca'}]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Aquí estamos buscando cualquier forma del verbo comprar\n",
        "# seguida de un sustantivo y un adjetivo,\n",
        "# como por ejemplo: \"comprando camisetas blancas\"\n",
        "[{\"LEMMA\": \"comprar\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"ADJ\"}]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-NlJdKkS5dz",
        "outputId": "a7d29d46-7cbe-4442-aece-e7cb567b06bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'LEMMA': 'comprar'}, {'POS': 'NOUN'}, {'POS': 'ADJ'}]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Usando el matcher"
      ],
      "metadata": {
        "id": "vGqLXHlLTp3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos el matcher\n",
        "from spacy.matcher import Matcher"
      ],
      "metadata": {
        "id": "kI8AsvPCTk8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iniciamos el matcher con un vocabulario compartido\n",
        "matcher = Matcher(nlp.vocab)"
      ],
      "metadata": {
        "id": "-DdNqATuTuIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Ejemplo 1"
      ],
      "metadata": {
        "id": "R3BlwbQRV9ff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Añadimos el patrón al matcher\n",
        "patron1 = [{\"POS\": \"NOUN\"}, {\"POS\": \"ADJ\"}]\n",
        "matcher.add(\"CAMISETAS\", [patron1])"
      ],
      "metadata": {
        "id": "C_5cI3TPT1JS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc4 = nlp(\"El otro día mi madre me compró una camiseta blanca en aquella tienda\")"
      ],
      "metadata": {
        "id": "MW_Rkip5UB0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LLamamos al matcher sobre el doc creado anteriormente\n",
        "matches = matcher(doc4)"
      ],
      "metadata": {
        "id": "zD0ysu0sUIPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for match_id, start, end in matches:\n",
        "    # Obtenemos los resultados\n",
        "    matched_span = doc4[start:end]\n",
        "    print(matched_span.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKd1-6g-UJrw",
        "outputId": "02ff4bf1-0799-4b47-aa06-5010a6a8a5ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "camiseta blanca\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Ejemplo 2"
      ],
      "metadata": {
        "id": "FhSjuMgIWC0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos el patrón\n",
        "patron2 = [\n",
        "    {'IS_PUNCT': True},\n",
        "    {'LOWER': 'españa'},\n",
        "    {'LEMMA': 'ganar'},\n",
        "    {'LOWER': 'el'},\n",
        "    {'LOWER': 'mundial'},\n",
        "    {'LOWER': 'en'},\n",
        "    {'IS_DIGIT': True},\n",
        "    {'IS_PUNCT': True}]"
      ],
      "metadata": {
        "id": "uhZ6Q56gUjEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lo añadimos al matcher\n",
        "matcher.add(\"FUTBOL\", [patron2])"
      ],
      "metadata": {
        "id": "IBMIOSx4Zge9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc5 = nlp(\"¡España ganó el mundial en 2010! Qué recuerdos\")"
      ],
      "metadata": {
        "id": "aVqE8ObWWTLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matches = matcher(doc5)"
      ],
      "metadata": {
        "id": "KE6G_es6W2dP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprimimos los resultados que coincidan\n",
        "for match_id, start, end in matches:\n",
        "    matched_span = doc5[start:end]\n",
        "    print(matched_span.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGP8-YD8W4W_",
        "outputId": "e06cf003-6a11-41fe-e374-756484e446c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¡España ganó el mundial en 2010!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Ejemplo 3"
      ],
      "metadata": {
        "id": "MaaDdJOdZ7i_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos un patron\n",
        "patron3 = [\n",
        "    {\"LEMMA\": \"comprar\"},\n",
        "    {\"POS\": \"DET\", \"OP\": \"?\"},  # opcional: encuentra 0 o 1 ocurrencias\n",
        "    {\"POS\": \"NOUN\"}\n",
        "]"
      ],
      "metadata": {
        "id": "1TK6cubkZ9ZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lo añadimos al matcher\n",
        "matcher.add(\"COCHES\", [patron3])"
      ],
      "metadata": {
        "id": "L4k9ghGiaS5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc6 = nlp('La semana pasada me compré el coche de mis sueños, ayer le a compré cosas para decorar')"
      ],
      "metadata": {
        "id": "5uKvJ0D9aVg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matches = matcher(doc6)"
      ],
      "metadata": {
        "id": "FqQ06Te1ahBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprimimos los resultados que coincidan\n",
        "for match_id, start, end in matches:\n",
        "    matched_span = doc6[start:end]\n",
        "    print(matched_span.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjhSDNX8a1ug",
        "outputId": "10c64736-fb5d-4809-886b-1bb5fd6bb16e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "semana pasada\n",
            "compré el coche\n",
            "compré cosas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Análisis de datos a gran escala"
      ],
      "metadata": {
        "id": "VX-kH3VNiguB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Estructuras de datos"
      ],
      "metadata": {
        "id": "iKDLFuHtin7K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Vocabulario compartido"
      ],
      "metadata": {
        "id": "8EoHH7KxlVcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Añadimos la palabra coche\n",
        "nlp.vocab.strings.add(\"coche\")\n",
        "\n",
        "# Añadimos el hash de la palabra coche\n",
        "coche_hash = nlp.vocab.strings[\"coche\"]\n",
        "\n",
        "# Mostramos el has\n",
        "coche_string = nlp.vocab.strings[coche_hash]\n",
        "\n",
        "# Mostramos el string y su hash\n",
        "print(coche_string, ':', coche_hash)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPtGz6QRik3i",
        "outputId": "82686764-9843-43dc-974f-8f3450c82b9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "coche : 6709559885411883416\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Podemos obtener el string a través del hash\n",
        "string = nlp.vocab.strings[6709559885411883416]\n",
        "\n",
        "print(string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRyvMisai5ja",
        "outputId": "821a976e-ae2e-4b2b-9c4f-0de6b0c36d35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "coche\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Podemos buscar el string / hash a través de un doc\n",
        "doc7 = nlp(\"Me he comprado un coche nuevo\")\n",
        "\n",
        "print(\"Hash:\", nlp.vocab.strings[\"coche\"])\n",
        "print(\"String:\", nlp.vocab.strings[6709559885411883416])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MHcm0UyjBCD",
        "outputId": "37b1065d-7c42-4ad0-b85e-193e3fb3d498"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hash: 6709559885411883416\n",
            "String: coche\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Lexemas"
      ],
      "metadata": {
        "id": "Wl-AVgl7lXiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lexema = nlp.vocab[\"coche\"]\n",
        "\n",
        "# Imprime en pantalla los atributos léxicos\n",
        "print(lexema.text, lexema.orth, lexema.is_alpha)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfSxPHQ0kLQP",
        "outputId": "be3a2520-3cf2-494e-aa76-eb7ed61e992a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "coche 6709559885411883416 True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Doc"
      ],
      "metadata": {
        "id": "sJuov-dbmW95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos la clase Doc\n",
        "from spacy.tokens import Doc"
      ],
      "metadata": {
        "id": "Odo9ZjUZlPFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Las palabras y espacios que usaremos para crear el doc\n",
        "palabras = [\"¡\" , \"Me\", \"llamo\", \"Javier\", \"!\"]\n",
        "espacios = [False, True, True, False, True]\n",
        "\n",
        "# Crea un doc manualmente\n",
        "doc8 = Doc(nlp.vocab, words=palabras, spaces=espacios)"
      ],
      "metadata": {
        "id": "hJs_n9DmmaZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(doc8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeuvOEn3moUo",
        "outputId": "c4ef02eb-14e2-4778-b8fb-e66fe60f636b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¡Me llamo Javier! \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Span"
      ],
      "metadata": {
        "id": "K8P_c9ycm_yY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos la clase Span\n",
        "from spacy.tokens import Span"
      ],
      "metadata": {
        "id": "nqkXjfmsnDaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crea un span manualmente\n",
        "span = Span(doc8, 1, 3)\n",
        "\n",
        "# Crea un span con un label\n",
        "span_with_label = Span(doc8, 1, 3, label=\"SALUDO\")\n",
        "\n",
        "# Añade el span a los doc.ents\n",
        "doc8.ents = [span_with_label]"
      ],
      "metadata": {
        "id": "ZGJbGn8Vne9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "span"
      ],
      "metadata": {
        "id": "Ogal5NJ6nfSB",
        "outputId": "66f86f2b-2923-4e8e-c749-721ae16d0816",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Me llamo"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word vectors y similitud semántica"
      ],
      "metadata": {
        "id": "H-tpL4L-I3Mm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download es_core_news_md"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4Jzv9aWJLBl",
        "outputId": "fbcd41f7-8f93-47d4-e604-fad43913e221"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "2023-03-07 17:36:20.157300: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-03-07 17:36:20.157436: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-03-07 17:36:20.157460: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2023-03-07 17:36:22.027831: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting es-core-news-md==3.4.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_md-3.4.0/es_core_news_md-3.4.0-py3-none-any.whl (42.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from es-core-news-md==3.4.0) (3.4.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (1.10.5)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (2.0.7)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (0.7.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (1.0.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (4.64.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (6.3.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (3.0.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (3.1.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (1.22.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (1.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (8.1.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (3.0.8)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (2.4.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (2.25.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (23.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (4.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (1.26.14)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (4.0.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->es-core-news-md==3.4.0) (2.1.2)\n",
            "Installing collected packages: es-core-news-md\n",
            "Successfully installed es-core-news-md-3.4.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_md')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos un modelo más grande\n",
        "nlp = spacy.load(\"es_core_news_md\")"
      ],
      "metadata": {
        "id": "-nedi2fEI4Do"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos 2 docs y los comparamos\n",
        "doc9 = nlp(\"Me he comprado una camiseta verde\")\n",
        "doc10 = nlp(\"Me compré una camiseta blanca\")\n",
        "print(doc9.similarity(doc10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9B3PebZI9yr",
        "outputId": "600f0dc4-ccba-496e-bc27-1deab952732d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7307724459016532\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparamos dos tokens\n",
        "doc11 = nlp(\"Me gustan los coches y las motos\")\n",
        "token1 = doc11[3]\n",
        "token2 = doc11[6]\n",
        "print(token1.similarity(token2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkMsCNnIJjw6",
        "outputId": "a0cccf75-d194-4f17-b437-e4452f94ee77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7295453548431396\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparamos un doc con un token\n",
        "doc12 = nlp(\"Me gustan los coches\")\n",
        "token = nlp(\"camiseta\")[0]\n",
        "\n",
        "print(doc12.similarity(token))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wYGFRT2J2Jo",
        "outputId": "bf5f843e-dfea-458b-d8e1-30cd0d7db7f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.12287714437993383\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparamos un span con un documento\n",
        "span = nlp(\"Me gustan los coches rojos\")[3:5]\n",
        "doc13 = nlp(\"Ferrari hace coches\")\n",
        "\n",
        "print(span.similarity(doc13))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjdIwiLbKGQZ",
        "outputId": "86bcbb50-9775-41c9-8783-e0b2f6de3c42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.534885437177009\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combinando modelos y reglas"
      ],
      "metadata": {
        "id": "hVoLcFFCMxtl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos el PhraseMatcher\n",
        "from spacy.matcher import PhraseMatcher"
      ],
      "metadata": {
        "id": "KVyYeGP2M0nl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iniciamos el matcher con un vocabulario compartido\n",
        "matcher = PhraseMatcher(nlp.vocab)"
      ],
      "metadata": {
        "id": "WhO4cd1QM4mr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos el patron que queremos buscar\n",
        "patron = nlp(\"coche rojo\")\n",
        "\n",
        "# Añadimos el patron al matcher\n",
        "matcher.add(\"COCHE\", [patron])\n",
        "\n",
        "# Creamos un doc\n",
        "doc14 = nlp(\"Me compré el otro dia mi primer coche rojo\")"
      ],
      "metadata": {
        "id": "i4ZwqOhWM_vt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprimimos los resultados\n",
        "for match_id, start, end in matcher(doc14):\n",
        "    span = doc14[start:end]\n",
        "    print(span.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaFTROrGNFAd",
        "outputId": "3c8ccef8-be43-4b78-9891-d468e900ca51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "coche rojo\n"
          ]
        }
      ]
    }
  ]
}